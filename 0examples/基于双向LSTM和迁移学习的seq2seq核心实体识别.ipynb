{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spaces.ac.cn/archives/3942"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于迁移学习和双向LSTM的核心实体识别\n",
    "\n",
    "迁移学习体现在：\n",
    "1、用训练语料和测试语料一起训练Word2Vec，使得词向量本捕捉了测试语料的语义；\n",
    "2、用训练语料训练模型；\n",
    "3、得到模型后，对测试语料预测，把预测结果跟训练语料一起训练新的模型；\n",
    "4、用新的模型预测，模型效果会有一定提升；\n",
    "5、对比两次预测结果，如果两次预测结果都一样，那说明这个预测结果很有可能是对的，用这部分“很有可能是对的”的测试结果来训练模型；\n",
    "6、用更新的模型预测；\n",
    "7、如果你愿意，可以继续重复第4、5、6步。\n",
    "\n",
    "双向LSTM的思路：\n",
    "1、分词；\n",
    "2、转换为5tag标注问题（0:非核心实体，1:单词的核心实体，2:多词核心实体的首词，3:多词核心实体的中间部分，4:多词核心实体的末词）；\n",
    "3、通过双向LSTM，直接对输入句子输出预测标注序列；\n",
    "4、通过viterbi算法来获得标注结果；\n",
    "5、因为常规的LSTM存在后面的词比前面的词更重要的弊端，因此用双向LSTM。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_json('/home/ian/code/github/data/data.json') #训练数据已经被预处理成为标准json格式\n",
    "d.index = range(len(d)) #重新定义一下索引，当然这只是优化显示效果\n",
    "word_size = 128 #词向量维度\n",
    "maxlen = 80 #句子截断长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12445, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>core_entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿佐藤健（Satoh  Takeru），1989年3月21日出生于日本埼玉县埼玉市，日本演员。</td>\n",
       "      <td>[佐藤健]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>在近现代政治中，左派是指社会中维护社会中下层利益，支持改变旧的不合理社会秩序，创造更为平等的...</td>\n",
       "      <td>[左派]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>《罪恶城市》和《Original  Gangstaz》一样是一款黑帮主题的角色扮演类游戏，虽...</td>\n",
       "      <td>[罪恶城市, Original  Gangstaz]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>朱明峰山的风景很美丽的，树木茂盛，古树参天，空气清新，奇花异草遍布山间，山顶的山脉耸立，是很...</td>\n",
       "      <td>[朱明峰山]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>《雪》是张艺军演唱的一首歌曲。</td>\n",
       "      <td>[雪]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0    ﻿佐藤健（Satoh  Takeru），1989年3月21日出生于日本埼玉县埼玉市，日本演员。   \n",
       "1  在近现代政治中，左派是指社会中维护社会中下层利益，支持改变旧的不合理社会秩序，创造更为平等的...   \n",
       "2  《罪恶城市》和《Original  Gangstaz》一样是一款黑帮主题的角色扮演类游戏，虽...   \n",
       "3  朱明峰山的风景很美丽的，树木茂盛，古树参天，空气清新，奇花异草遍布山间，山顶的山脉耸立，是很...   \n",
       "4                                    《雪》是张艺军演唱的一首歌曲。   \n",
       "\n",
       "                  core_entity  \n",
       "0                       [佐藤健]  \n",
       "1                        [左派]  \n",
       "2  [罪恶城市, Original  Gangstaz]  \n",
       "3                      [朱明峰山]  \n",
       "4                         [雪]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "修改分词函数，主要是：\n",
    "1、英文和数字部分不分词，直接返回；\n",
    "2、双书名号里边的内容不分词；\n",
    "3、双引号里边如果是十字以内的内容不分词；\n",
    "4、超出范围内的字符全部替换为空格；\n",
    "5、分词使用结巴分词，并关闭新词发现功能。\n",
    "'''\n",
    "#匹配数字、大小写字母、空格、.或者书名号内的字符，或者匹配中文双引号内的字符\n",
    "not_cuts = re.compile(u'([\\da-zA-Z \\.]+)|《(.*?)》|“(.{1,10})”')\n",
    "#匹配开始以汉字、数字、大小写字母、书名号、中英文括号、中文双引号、·、英文句号\n",
    "re_replace = re.compile(u'[^\\u4e00-\\u9fa50-9a-zA-Z《》\\(\\)（）“”·\\.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   你   好'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_replace.sub(' ','---你。。。好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(4, 16), match='Satoh Takeru'>\n",
      "<_sre.SRE_Match object; span=(18, 22), match='1989'>\n",
      "<_sre.SRE_Match object; span=(23, 24), match='3'>\n",
      "<_sre.SRE_Match object; span=(25, 27), match='21'>\n"
     ]
    }
   ],
   "source": [
    "for i in not_cuts.finditer('佐藤健（Satoh Takeru），1989年3月21日出生于日本埼玉县埼玉市，日本演员。'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 3), match='《雪》'>\n"
     ]
    }
   ],
   "source": [
    "for i in not_cuts.finditer('《雪》是张艺军演唱的一首歌曲。'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycut(s):\n",
    "    result = []\n",
    "    j = 0\n",
    "    s = re_replace.sub(' ', s)#超出范围内的字符全部替换为空格；\n",
    "    for i in not_cuts.finditer(s):\n",
    "        result.extend(jieba.lcut(s[j:i.start()], HMM=False))\n",
    "        if s[i.start()] in [u'《', u'“']:\n",
    "            result.extend([s[i.start()], s[i.start()+1:i.end()-1], s[i.end()-1]])\n",
    "        else:\n",
    "            result.append(s[i.start():i.end()])\n",
    "        j = i.end()\n",
    "    result.extend(jieba.lcut(s[j:], HMM=False))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.585 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>core_entity</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿佐藤健（Satoh  Takeru），1989年3月21日出生于日本埼玉县埼玉市，日本演员。</td>\n",
       "      <td>[佐藤健]</td>\n",
       "      <td>[ , 佐藤, 健, （, Satoh  Takeru, ）,  1989, 年, 3, 月...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>在近现代政治中，左派是指社会中维护社会中下层利益，支持改变旧的不合理社会秩序，创造更为平等的...</td>\n",
       "      <td>[左派]</td>\n",
       "      <td>[在, 近现代, 政治, 中,  , 左派, 是, 指, 社会, 中, 维护, 社会, 中下...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>《罪恶城市》和《Original  Gangstaz》一样是一款黑帮主题的角色扮演类游戏，虽...</td>\n",
       "      <td>[罪恶城市, Original  Gangstaz]</td>\n",
       "      <td>[《, 罪恶城市, 》, 和, 《, Original  Gangstaz, 》, 一样, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>朱明峰山的风景很美丽的，树木茂盛，古树参天，空气清新，奇花异草遍布山间，山顶的山脉耸立，是很...</td>\n",
       "      <td>[朱明峰山]</td>\n",
       "      <td>[朱, 明, 峰山, 的, 风景, 很, 美丽, 的,  , 树木, 茂盛,  , 古树, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>《雪》是张艺军演唱的一首歌曲。</td>\n",
       "      <td>[雪]</td>\n",
       "      <td>[《, 雪, 》, 是, 张, 艺, 军, 演唱, 的, 一首, 歌曲,  ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0    ﻿佐藤健（Satoh  Takeru），1989年3月21日出生于日本埼玉县埼玉市，日本演员。   \n",
       "1  在近现代政治中，左派是指社会中维护社会中下层利益，支持改变旧的不合理社会秩序，创造更为平等的...   \n",
       "2  《罪恶城市》和《Original  Gangstaz》一样是一款黑帮主题的角色扮演类游戏，虽...   \n",
       "3  朱明峰山的风景很美丽的，树木茂盛，古树参天，空气清新，奇花异草遍布山间，山顶的山脉耸立，是很...   \n",
       "4                                    《雪》是张艺军演唱的一首歌曲。   \n",
       "\n",
       "                  core_entity  \\\n",
       "0                       [佐藤健]   \n",
       "1                        [左派]   \n",
       "2  [罪恶城市, Original  Gangstaz]   \n",
       "3                      [朱明峰山]   \n",
       "4                         [雪]   \n",
       "\n",
       "                                               words  \n",
       "0  [ , 佐藤, 健, （, Satoh  Takeru, ）,  1989, 年, 3, 月...  \n",
       "1  [在, 近现代, 政治, 中,  , 左派, 是, 指, 社会, 中, 维护, 社会, 中下...  \n",
       "2  [《, 罪恶城市, 》, 和, 《, Original  Gangstaz, 》, 一样, ...  \n",
       "3  [朱, 明, 峰山, 的, 风景, 很, 美丽, 的,  , 树木, 茂盛,  , 古树, ...  \n",
       "4            [《, 雪, 》, 是, 张, 艺, 军, 演唱, 的, 一首, 歌曲,  ]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['words'] = d['content'].apply(mycut) #分词\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(k): #将输出结果转换为标签序列\n",
    "    s = d['words'][k]\n",
    "    r = ['0']*len(s)\n",
    "    for i in range(len(s)):\n",
    "        for j in d['core_entity'][k]:\n",
    "            if s[i] in j:\n",
    "                r[i] = '1'\n",
    "                break\n",
    "    s = ''.join(r)\n",
    "    r = [0]*len(s)\n",
    "    for i in re.finditer('1+', s):\n",
    "        if i.end() - i.start() > 1:\n",
    "            r[i.start()] = 2#2表示≥2个字的核心实体的开始\n",
    "            r[i.end()-1] = 4#4表示≥2个字的核心实体的结尾\n",
    "            for j in range(i.start()+1, i.end()-1):\n",
    "                r[j] = 3#3表示≥2个字的核心实体的中间\n",
    "        else:\n",
    "            r[i.start()] = 1#1表示1个字的核心实体\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "222it [00:00, 2199.37it/s]\u001b[A\n",
      "508it [00:00, 2528.43it/s]\u001b[A\n",
      "765it [00:00, 2537.33it/s]\u001b[A\n",
      "1026it [00:00, 2553.44it/s]\u001b[A\n",
      "1277it [00:00, 2543.29it/s]\u001b[A\n",
      "1495it [00:00, 2481.08it/s]\u001b[A\n",
      "1709it [00:00, 2378.90it/s]\u001b[A\n",
      "1915it [00:00, 2337.98it/s]\u001b[A\n",
      "2195it [00:00, 2387.50it/s]\u001b[A\n",
      "2447it [00:01, 2400.47it/s]\u001b[A\n",
      "2717it [00:01, 2427.03it/s]\u001b[A\n",
      "2993it [00:01, 2453.83it/s]\u001b[A\n",
      "3291it [00:01, 2493.80it/s]\u001b[A\n",
      "3580it [00:01, 2521.15it/s]\u001b[A\n",
      "3858it [00:01, 2537.88it/s]\u001b[A\n",
      "4144it [00:01, 2557.09it/s]\u001b[A\n",
      "4422it [00:01, 2568.67it/s]\u001b[A\n",
      "4707it [00:01, 2583.69it/s]\u001b[A\n",
      "5005it [00:01, 2604.29it/s]\u001b[A\n",
      "5318it [00:02, 2630.19it/s]\u001b[A\n",
      "5612it [00:02, 2641.94it/s]\u001b[A\n",
      "5904it [00:02, 2651.34it/s]\u001b[A\n",
      "6194it [00:02, 2661.98it/s]\u001b[A\n",
      "6484it [00:02, 2668.59it/s]\u001b[A\n",
      "6772it [00:02, 2671.43it/s]\u001b[A\n",
      "7056it [00:02, 2671.92it/s]\u001b[A\n",
      "7336it [00:02, 2676.34it/s]\u001b[A\n",
      "7615it [00:02, 2675.69it/s]\u001b[A\n",
      "7898it [00:02, 2678.58it/s]\u001b[A\n",
      "8176it [00:03, 2655.73it/s]\u001b[A\n",
      "8477it [00:03, 2666.94it/s]\u001b[A\n",
      "8771it [00:03, 2675.05it/s]\u001b[A\n",
      "9049it [00:03, 2677.88it/s]\u001b[A\n",
      "9327it [00:03, 2664.70it/s]\u001b[A\n",
      "9590it [00:03, 2651.62it/s]\u001b[A\n",
      "9854it [00:03, 2650.94it/s]\u001b[A\n",
      "10110it [00:03, 2647.49it/s]\u001b[A\n",
      "10371it [00:03, 2646.46it/s]\u001b[A\n",
      "10637it [00:04, 2646.48it/s]\u001b[A\n",
      "10907it [00:04, 2647.51it/s]\u001b[A\n",
      "11192it [00:04, 2652.14it/s]\u001b[A\n",
      "11489it [00:04, 2659.20it/s]\u001b[A\n",
      "11766it [00:04, 2657.84it/s]\u001b[A\n",
      "12038it [00:04, 2657.27it/s]\u001b[A\n",
      "12318it [00:04, 2660.33it/s]\u001b[A\n",
      "12445it [00:04, 2658.82it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "d['label'] = list(map(label, tqdm(iter(d.index)))) #输出tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>core_entity</th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿佐藤健（Satoh  Takeru），1989年3月21日出生于日本埼玉县埼玉市，日本演员。</td>\n",
       "      <td>[佐藤健]</td>\n",
       "      <td>[ , 佐藤, 健, （, Satoh  Takeru, ）,  1989, 年, 3, 月...</td>\n",
       "      <td>[0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>在近现代政治中，左派是指社会中维护社会中下层利益，支持改变旧的不合理社会秩序，创造更为平等的...</td>\n",
       "      <td>[左派]</td>\n",
       "      <td>[在, 近现代, 政治, 中,  , 左派, 是, 指, 社会, 中, 维护, 社会, 中下...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>《罪恶城市》和《Original  Gangstaz》一样是一款黑帮主题的角色扮演类游戏，虽...</td>\n",
       "      <td>[罪恶城市, Original  Gangstaz]</td>\n",
       "      <td>[《, 罪恶城市, 》, 和, 《, Original  Gangstaz, 》, 一样, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>朱明峰山的风景很美丽的，树木茂盛，古树参天，空气清新，奇花异草遍布山间，山顶的山脉耸立，是很...</td>\n",
       "      <td>[朱明峰山]</td>\n",
       "      <td>[朱, 明, 峰山, 的, 风景, 很, 美丽, 的,  , 树木, 茂盛,  , 古树, ...</td>\n",
       "      <td>[2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>《雪》是张艺军演唱的一首歌曲。</td>\n",
       "      <td>[雪]</td>\n",
       "      <td>[《, 雪, 》, 是, 张, 艺, 军, 演唱, 的, 一首, 歌曲,  ]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0    ﻿佐藤健（Satoh  Takeru），1989年3月21日出生于日本埼玉县埼玉市，日本演员。   \n",
       "1  在近现代政治中，左派是指社会中维护社会中下层利益，支持改变旧的不合理社会秩序，创造更为平等的...   \n",
       "2  《罪恶城市》和《Original  Gangstaz》一样是一款黑帮主题的角色扮演类游戏，虽...   \n",
       "3  朱明峰山的风景很美丽的，树木茂盛，古树参天，空气清新，奇花异草遍布山间，山顶的山脉耸立，是很...   \n",
       "4                                    《雪》是张艺军演唱的一首歌曲。   \n",
       "\n",
       "                  core_entity  \\\n",
       "0                       [佐藤健]   \n",
       "1                        [左派]   \n",
       "2  [罪恶城市, Original  Gangstaz]   \n",
       "3                      [朱明峰山]   \n",
       "4                         [雪]   \n",
       "\n",
       "                                               words  \\\n",
       "0  [ , 佐藤, 健, （, Satoh  Takeru, ）,  1989, 年, 3, 月...   \n",
       "1  [在, 近现代, 政治, 中,  , 左派, 是, 指, 社会, 中, 维护, 社会, 中下...   \n",
       "2  [《, 罪恶城市, 》, 和, 《, Original  Gangstaz, 》, 一样, ...   \n",
       "3  [朱, 明, 峰山, 的, 风景, 很, 美丽, 的,  , 树木, 茂盛,  , 古树, ...   \n",
       "4            [《, 雪, 》, 是, 张, 艺, 军, 演唱, 的, 一首, 歌曲,  ]   \n",
       "\n",
       "                                               label  \n",
       "0  [0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4               [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机打乱数据\n",
    "idx = list(range(len(d)))\n",
    "d.index = idx\n",
    "np.random.shuffle(idx)#Modify a sequence in-place by shuffling its contents.\n",
    "d = d.loc[idx]\n",
    "d.index = range(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "用gensim来训练Word2Vec：\n",
    "1、联合训练语料和测试语料一起训练；\n",
    "2、经过测试用skip gram效果会好些。\n",
    "'''\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-08 16:54:50,061 : INFO : collecting all words and their counts\n",
      "2018-08-08 16:54:50,064 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-08 16:54:50,149 : INFO : PROGRESS: at sentence #10000, processed 312348 words, keeping 37036 word types\n",
      "2018-08-08 16:54:50,164 : INFO : collected 42029 word types from a corpus of 390502 raw words and 12445 sentences\n",
      "2018-08-08 16:54:50,166 : INFO : Loading a fresh vocabulary\n",
      "2018-08-08 16:54:50,278 : INFO : effective_min_count=1 retains 42029 unique words (100% of original 42029, drops 0)\n",
      "2018-08-08 16:54:50,279 : INFO : effective_min_count=1 leaves 390502 word corpus (100% of original 390502, drops 0)\n",
      "2018-08-08 16:54:50,358 : INFO : deleting the raw counts dictionary of 42029 items\n",
      "2018-08-08 16:54:50,360 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2018-08-08 16:54:50,361 : INFO : downsampling leaves estimated 303299 word corpus (77.7% of prior 390502)\n",
      "2018-08-08 16:54:50,428 : INFO : estimated required memory for 42029 words and 128 dimensions: 64052196 bytes\n",
      "2018-08-08 16:54:50,429 : INFO : resetting layer weights\n",
      "2018-08-08 16:54:50,762 : INFO : training model with 20 workers on 42029 vocabulary and 128 features, using sg=1 hs=0 sample=0.001 negative=8 window=8\n",
      "2018-08-08 16:54:51,222 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:51,236 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:51,244 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:51,252 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:51,254 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:51,257 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:51,270 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:51,276 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:51,283 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:51,285 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:51,287 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:51,290 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:51,297 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:51,309 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:51,315 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:51,319 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:51,324 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:51,360 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:51,374 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:51,396 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:51,398 : INFO : EPOCH - 1 : training on 390502 raw words (303320 effective words) took 0.6s, 488673 effective words/s\n",
      "2018-08-08 16:54:51,860 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:51,863 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:51,869 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:51,875 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:51,884 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:51,888 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:51,900 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:51,902 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:51,911 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:51,918 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:51,920 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:51,936 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:51,937 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:51,939 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:51,940 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:51,941 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:51,943 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:51,975 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:51,986 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:51,994 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:51,995 : INFO : EPOCH - 2 : training on 390502 raw words (303165 effective words) took 0.6s, 525833 effective words/s\n",
      "2018-08-08 16:54:52,454 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:52,456 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:52,460 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:52,462 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:52,468 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:52,475 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:52,481 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:52,486 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:52,516 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:52,523 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:52,525 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:52,541 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:52,542 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:52,544 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:52,546 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:52,553 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:52,554 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:52,557 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:52,578 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:52,603 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:52,605 : INFO : EPOCH - 3 : training on 390502 raw words (303536 effective words) took 0.6s, 516286 effective words/s\n",
      "2018-08-08 16:54:53,043 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:53,051 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:53,066 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:53,067 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:53,069 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:53,083 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:53,089 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:53,091 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:53,107 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:53,122 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:53,125 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:53,129 : INFO : worker thread finished; awaiting finish of 8 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-08 16:54:53,139 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:53,149 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:53,162 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:53,165 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:53,180 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:53,196 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:53,200 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:53,207 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:53,208 : INFO : EPOCH - 4 : training on 390502 raw words (303228 effective words) took 0.6s, 518552 effective words/s\n",
      "2018-08-08 16:54:53,635 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:53,646 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:53,658 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:53,683 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:53,694 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:53,696 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:53,697 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:53,702 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:53,708 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:53,711 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:53,724 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:53,741 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:53,742 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:53,748 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:53,759 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:53,760 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:53,775 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:53,777 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:53,787 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:53,823 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:53,824 : INFO : EPOCH - 5 : training on 390502 raw words (303263 effective words) took 0.6s, 503629 effective words/s\n",
      "2018-08-08 16:54:54,249 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:54,250 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:54,261 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:54,274 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:54,285 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:54,311 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:54,313 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:54,318 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:54,321 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:54,330 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:54,336 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:54,340 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:54,349 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:54,350 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:54,380 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:54,382 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:54,392 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:54,394 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:54,397 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:54,414 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:54,415 : INFO : EPOCH - 6 : training on 390502 raw words (303308 effective words) took 0.6s, 533154 effective words/s\n",
      "2018-08-08 16:54:54,893 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:54,902 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:54,908 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:54,914 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:54,922 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:54,924 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:54,930 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:54,933 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:54,947 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:54,951 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:54,976 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:54,981 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:54,982 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:54,983 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:54,988 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:54,996 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:54,998 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:54,999 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:55,024 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:55,035 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:55,037 : INFO : EPOCH - 7 : training on 390502 raw words (303281 effective words) took 0.6s, 510989 effective words/s\n",
      "2018-08-08 16:54:55,473 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:55,484 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:55,486 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:55,492 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:55,511 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:55,512 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:55,522 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:55,536 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:55,541 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:55,542 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:55,544 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:55,554 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:55,560 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:55,567 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:55,572 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:55,592 : INFO : worker thread finished; awaiting finish of 4 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-08 16:54:55,594 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:55,598 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:55,602 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:55,606 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:55,608 : INFO : EPOCH - 8 : training on 390502 raw words (303112 effective words) took 0.6s, 550907 effective words/s\n",
      "2018-08-08 16:54:56,090 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:56,092 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:56,096 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:56,101 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:56,118 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:56,142 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:56,147 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:56,151 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:56,158 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:56,163 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:56,164 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:56,171 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:56,202 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:56,216 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:56,218 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:56,219 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:56,227 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:56,228 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:56,242 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:56,255 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:56,256 : INFO : EPOCH - 9 : training on 390502 raw words (303427 effective words) took 0.6s, 479112 effective words/s\n",
      "2018-08-08 16:54:56,676 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:56,684 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:56,694 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:56,712 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:56,716 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:56,723 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:56,727 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:56,735 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:56,740 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:56,745 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:56,754 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:56,767 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:56,770 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:56,772 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:56,778 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:56,784 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:56,785 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:56,795 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:56,844 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:56,848 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:56,849 : INFO : EPOCH - 10 : training on 390502 raw words (303144 effective words) took 0.6s, 523264 effective words/s\n",
      "2018-08-08 16:54:57,269 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:57,278 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:57,305 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:57,324 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:57,331 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:57,355 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:57,357 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:57,373 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:57,380 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:57,387 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:57,388 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:57,399 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:57,402 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:57,418 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:57,422 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:57,422 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:57,423 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:57,450 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:57,458 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:57,486 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:57,487 : INFO : EPOCH - 11 : training on 390502 raw words (303262 effective words) took 0.6s, 492628 effective words/s\n",
      "2018-08-08 16:54:57,946 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:57,953 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:57,956 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:57,982 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:58,002 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:58,013 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:58,020 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:58,023 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:58,039 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:58,046 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:58,062 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:58,073 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:58,076 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:58,077 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:58,078 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:58,080 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:58,082 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:58,083 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:58,085 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:58,092 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-08 16:54:58,093 : INFO : EPOCH - 12 : training on 390502 raw words (303431 effective words) took 0.6s, 518632 effective words/s\n",
      "2018-08-08 16:54:58,538 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:58,559 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:58,565 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:58,571 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:58,578 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:58,589 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:58,596 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:58,597 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:58,602 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:58,607 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:58,618 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:58,619 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:58,633 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:58,637 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:58,640 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:58,648 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:58,658 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:58,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:58,677 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:58,682 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:58,683 : INFO : EPOCH - 13 : training on 390502 raw words (303416 effective words) took 0.6s, 532835 effective words/s\n",
      "2018-08-08 16:54:59,113 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:59,126 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:59,129 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:59,130 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:59,138 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:59,155 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:59,165 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:59,186 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:59,214 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:59,216 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:59,226 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:59,233 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:59,236 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:59,244 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:59,249 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:59,250 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:59,251 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:59,253 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:59,259 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:59,281 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:59,282 : INFO : EPOCH - 14 : training on 390502 raw words (303169 effective words) took 0.6s, 524268 effective words/s\n",
      "2018-08-08 16:54:59,715 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:54:59,724 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:54:59,727 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:54:59,776 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:54:59,821 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:54:59,829 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:54:59,856 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:54:59,859 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:54:59,863 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:54:59,880 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:54:59,884 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:54:59,894 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:54:59,895 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:54:59,896 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:54:59,898 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:54:59,900 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:54:59,913 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:54:59,914 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:54:59,920 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:54:59,928 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:54:59,929 : INFO : EPOCH - 15 : training on 390502 raw words (303229 effective words) took 0.6s, 477945 effective words/s\n",
      "2018-08-08 16:55:00,394 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:55:00,397 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:55:00,400 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:55:00,401 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:55:00,414 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:55:00,415 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:55:00,435 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:55:00,439 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:55:00,446 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:55:00,461 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:55:00,462 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:55:00,480 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:55:00,481 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:55:00,483 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:55:00,485 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:55:00,493 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:55:00,497 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:55:00,500 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:55:00,501 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:55:00,521 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:55:00,522 : INFO : EPOCH - 16 : training on 390502 raw words (303032 effective words) took 0.6s, 527877 effective words/s\n",
      "2018-08-08 16:55:01,014 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:55:01,067 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:55:01,116 : INFO : worker thread finished; awaiting finish of 17 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-08 16:55:01,140 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:55:01,153 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:55:01,174 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:55:01,182 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:55:01,187 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:55:01,188 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:55:01,192 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:55:01,208 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:55:01,217 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:55:01,219 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:55:01,221 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:55:01,234 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:55:01,236 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:55:01,244 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:55:01,246 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:55:01,253 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:55:01,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:55:01,255 : INFO : EPOCH - 17 : training on 390502 raw words (303328 effective words) took 0.7s, 421667 effective words/s\n",
      "2018-08-08 16:55:01,753 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:55:01,791 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:55:01,797 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:55:01,806 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:55:01,821 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:55:01,831 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:55:01,837 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:55:01,849 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:55:01,868 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:55:01,872 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:55:01,875 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:55:01,890 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:55:01,891 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:55:01,900 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:55:01,913 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:55:01,919 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:55:01,921 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:55:01,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:55:01,936 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:55:01,946 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:55:01,947 : INFO : EPOCH - 18 : training on 390502 raw words (303401 effective words) took 0.7s, 453555 effective words/s\n",
      "2018-08-08 16:55:02,429 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:55:02,490 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:55:02,533 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:55:02,547 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:55:02,557 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:55:02,591 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:55:02,597 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:55:02,608 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:55:02,617 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:55:02,618 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:55:02,623 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:55:02,628 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:55:02,630 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:55:02,649 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:55:02,650 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:55:02,651 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:55:02,657 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:55:02,662 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:55:02,665 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:55:02,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:55:02,671 : INFO : EPOCH - 19 : training on 390502 raw words (303456 effective words) took 0.7s, 427167 effective words/s\n",
      "2018-08-08 16:55:03,156 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-08-08 16:55:03,198 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-08-08 16:55:03,252 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-08-08 16:55:03,268 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-08-08 16:55:03,278 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-08-08 16:55:03,283 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-08-08 16:55:03,284 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-08-08 16:55:03,289 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-08-08 16:55:03,294 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-08-08 16:55:03,296 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-08-08 16:55:03,303 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-08 16:55:03,315 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-08 16:55:03,316 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-08 16:55:03,318 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-08 16:55:03,333 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-08 16:55:03,339 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-08 16:55:03,340 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-08 16:55:03,342 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-08 16:55:03,358 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-08 16:55:03,359 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-08 16:55:03,360 : INFO : EPOCH - 20 : training on 390502 raw words (303253 effective words) took 0.7s, 449797 effective words/s\n",
      "2018-08-08 16:55:03,362 : INFO : training on a 7810040 raw words (6065761 effective words) took 12.6s, 481478 effective words/s\n",
      "2018-08-08 16:55:03,363 : INFO : saving Word2Vec object under word2vec_words_final.model, separately None\n",
      "2018-08-08 16:55:03,364 : INFO : not storing attribute vectors_norm\n",
      "2018-08-08 16:55:03,367 : INFO : not storing attribute cum_table\n",
      "2018-08-08 16:55:03,764 : INFO : saved word2vec_words_final.model\n",
      "2018-08-08 16:55:03,765 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 40s, sys: 573 ms, total: 2min 41s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2vec = gensim.models.Word2Vec(d['words'], \n",
    "                                  min_count=1, \n",
    "                                  size=word_size, \n",
    "                                  workers=20,\n",
    "                                  iter=20,\n",
    "                                  window=8,\n",
    "                                  negative=8,\n",
    "                                  sg=1)\n",
    "word2vec.save('word2vec_words_final.model')\n",
    "word2vec.init_sims(replace=True) #预先归一化，使得词向量不受尺度影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在进行第一次训练......\n"
     ]
    }
   ],
   "source": [
    "print('正在进行第一次训练......')\n",
    "\n",
    "'''\n",
    "用最新版本的Keras训练模型，使用GPU加速（我的是GTX 960）\n",
    "其中Bidirectional函数目前要在github版本才有\n",
    "'''\n",
    "from keras.layers import Dense, LSTM, Lambda, TimeDistributed, Input, Masking, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1 #通过L1正则项，使得输出更加稀疏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = Input(shape=(maxlen, word_size))\n",
    "mask = Masking(mask_value=0.)(sequence)\n",
    "blstm = Bidirectional(LSTM(64, return_sequences=True), merge_mode='sum')(mask)\n",
    "blstm = Bidirectional(LSTM(32, return_sequences=True), merge_mode='sum')(blstm)\n",
    "output = TimeDistributed(Dense(5, activation='softmax', activity_regularizer=l1(0.01)))(blstm)\n",
    "model = Model(inputs=sequence, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 80, 128)           0         \n",
      "_________________________________________________________________\n",
      "masking_3 (Masking)          (None, 80, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 80, 64)            98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 80, 32)            24832     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 80, 5)             165       \n",
      "=================================================================\n",
      "Total params: 123,813\n",
      "Trainable params: 123,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "gen_matrix实现从分词后的list来输出训练样本\n",
    "gen_target实现将输出序列转换为one hot形式的目标\n",
    "超过maxlen则截断，不足补0\n",
    "'''\n",
    "gen_matrix = lambda z: np.vstack((word2vec[z[:maxlen]], np.zeros((maxlen-len(z[:maxlen]), word_size))))\n",
    "gen_target = lambda z: np_utils.to_categorical(np.array(z[:maxlen] + [0]*(maxlen-len(z[:maxlen]))), 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从节省内存的角度，通过生成器的方式来训练\n",
    "def data_generator(data, targets, batch_size): \n",
    "    idx = np.arange(len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    batches = [idx[range(batch_size*i, min(len(data), batch_size*(i+1)))] for i in range(len(data)//batch_size+1)]\n",
    "    while True:\n",
    "        for i in batches:\n",
    "            xx, yy = np.array(map(gen_matrix, data[i])), np.array(map(gen_target, targets[i]))\n",
    "            yield (xx, yy)\n",
    "\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/installed/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/ian/installed/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=12445, epochs=200)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/installed/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installed/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installed/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# build batch logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m                     \u001b[0;31m# Handle data tensors support when no input given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0;31m# step-size = 1 for data tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit_generator(data_generator(d['words'], d['label'], batch_size), samples_per_epoch=len(d), nb_epoch=200)\n",
    "model.save_weights('words_seq2seq_final_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输出预测结果（原始数据，未整理）\n",
    "def predict_data(data, batch_size):\n",
    "    batches = [range(batch_size*i, min(len(data), batch_size*(i+1))) for i in range(len(data)/batch_size+1)]\n",
    "    p = model.predict(np.array(map(gen_matrix, data[batches[0]])), verbose=1)\n",
    "    for i in batches[1:]:\n",
    "        print min(i), 'done.'\n",
    "        p = np.vstack((p, model.predict(np.array(map(gen_matrix, data[i])), verbose=1)))\n",
    "    return p\n",
    "\n",
    "d['predict'] = list(predict_data(d['words'], batch_size))\n",
    "dd['predict'] = list(predict_data(dd['words'], batch_size))\n",
    "\n",
    "'''\n",
    "动态规划部分：\n",
    "1、zy是转移矩阵，用了对数概率；概率的数值是大概估计的，事实上，这个数值的精确意义不是很大。\n",
    "2、viterbi是动态规划算法。\n",
    "'''\n",
    "zy = {'00':0.15, \n",
    "      '01':0.15, \n",
    "      '02':0.7, \n",
    "      '10':1.0, \n",
    "      '23':0.5, \n",
    "      '24':0.5,\n",
    "      '33':0.5,\n",
    "      '34':0.5, \n",
    "      '40':1.0\n",
    "     }\n",
    "\n",
    "zy = {i:np.log(zy[i]) for i in zy.keys()}\n",
    "\n",
    "def viterbi(nodes):\n",
    "    paths = nodes[0]\n",
    "    for l in range(1,len(nodes)):\n",
    "        paths_ = paths.copy()\n",
    "        paths = {}\n",
    "        for i in nodes[l].keys():\n",
    "            nows = {}\n",
    "            for j in paths_.keys():\n",
    "                if j[-1]+i in zy.keys():\n",
    "                    nows[j+i]= paths_[j]+nodes[l][i]+zy[j[-1]+i]\n",
    "            k = np.argmax(nows.values())\n",
    "            paths[nows.keys()[k]] = nows.values()[k]\n",
    "    return paths.keys()[np.argmax(paths.values())]\n",
    "\n",
    "'''\n",
    "整理输出结果，即生成提交数据所需要的格式。\n",
    "整个过程包括：动态规划、结果提取。\n",
    "'''\n",
    "\n",
    "def predict(i):\n",
    "    nodes = [dict(zip(['0','1','2','3','4'], k)) for k in np.log(dd['predict'][i][:len(dd['words'][i])])]\n",
    "    r = viterbi(nodes)\n",
    "    result = []\n",
    "    words = dd['words'][i]\n",
    "    for j in re.finditer('2.*?4|1', r):\n",
    "        result.append((''.join(words[j.start():j.end()]), np.mean([nodes[k][r[k]] for k in range(j.start(),j.end())])))\n",
    "    if result:\n",
    "        result = pd.DataFrame(result)\n",
    "        return [result[0][result[1].argmax()]]\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "dd['core_entity'] = map(predict, tqdm(iter(dd.index), desc=u'第一次预测'))\n",
    "\n",
    "'''\n",
    "导出提交的JSON格式\n",
    "'''\n",
    "gen = lambda i:'[{\"content\": \"'+dd.iloc[i]['content']+'\", \"core_entity\": [\"'+''.join(dd.iloc[i]['core_entity'])+'\"]}]'\n",
    "ssss = map(gen, tqdm(range(len(dd))))\n",
    "result='\\n'.join(ssss)\n",
    "import codecs\n",
    "f=codecs.open('result1.txt', 'w', encoding='utf-8')\n",
    "f.write(result)\n",
    "f.close()\n",
    "import os\n",
    "os.system('rm result1.zip')\n",
    "os.system('zip result1.zip result1.txt')\n",
    "\n",
    "print u'正在进行第一次迁移学习......'\n",
    "\n",
    "'''\n",
    "开始迁移学习。\n",
    "'''\n",
    "\n",
    "def label(k): #将输出结果转换为标签序列\n",
    "    s = dd['words'][k]\n",
    "    r = ['0']*len(s)\n",
    "    for i in range(len(s)):\n",
    "        for j in dd['core_entity'][k]:\n",
    "            if s[i] in j:\n",
    "                r[i] = '1'\n",
    "                break\n",
    "    s = ''.join(r)\n",
    "    r = [0]*len(s)\n",
    "    for i in re.finditer('1+', s):\n",
    "        if i.end() - i.start() > 1:\n",
    "            r[i.start()] = 2\n",
    "            r[i.end()-1] = 4\n",
    "            for j in range(i.start()+1, i.end()-1):\n",
    "                r[j] = 3\n",
    "        else:\n",
    "            r[i.start()] = 1\n",
    "    return r\n",
    "\n",
    "dd['label'] = map(label, tqdm(iter(dd.index))) #输出tags\n",
    "\n",
    "'''\n",
    "将测试集和训练集一起放到模型中训练，\n",
    "其中测试集的样本权重设置为1，训练集为10\n",
    "'''\n",
    "w = np.array([1]*len(dd) + [10]*len(d))\n",
    "def data_generator(data, targets, batch_size): \n",
    "    idx = np.arange(len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    batches = [idx[range(batch_size*i, min(len(data), batch_size*(i+1)))] for i in range(len(data)/batch_size+1)]\n",
    "    while True:\n",
    "        for i in batches:\n",
    "            xx, yy = np.array(map(gen_matrix, data[i])), np.array(map(gen_target, targets[i]))\n",
    "            yield (xx, yy, w[i])\n",
    "\n",
    "history = model.fit_generator(data_generator(\n",
    "                                    dd[['words']].append(d[['words']], ignore_index=True)['words'], \n",
    "                                    dd[['label']].append(d[['label']], ignore_index=True)['label'], \n",
    "                                    batch_size), \n",
    "                              samples_per_epoch=len(dd)+len(d), \n",
    "                              nb_epoch=20)\n",
    "\n",
    "model.save_weights('words_seq2seq_final_2.model')\n",
    "d['predict'] = list(predict_data(d['words'], batch_size))\n",
    "dd['predict'] = list(predict_data(dd['words'], batch_size))\n",
    "dd['core_entity_2'] = map(predict, tqdm(iter(dd.index), desc=u'第一次迁移学习预测'))\n",
    "\n",
    "'''\n",
    "导出提交的JSON格式\n",
    "'''\n",
    "gen = lambda i:'[{\"content\": \"'+dd.iloc[i]['content']+'\", \"core_entity\": [\"'+''.join(dd.iloc[i]['core_entity_2'])+'\"]}]'\n",
    "ssss = map(gen, tqdm(range(len(dd))))\n",
    "result='\\n'.join(ssss)\n",
    "import codecs\n",
    "f=codecs.open('result2.txt', 'w', encoding='utf-8')\n",
    "f.write(result)\n",
    "f.close()\n",
    "import os\n",
    "os.system('rm result2.zip')\n",
    "os.system('zip result2.zip result2.txt')\n",
    "\n",
    "print u'正在进行第二次迁移学习......'\n",
    "\n",
    "'''\n",
    "开始迁移学习2。\n",
    "'''\n",
    "\n",
    "ddd = dd[dd['core_entity'] == dd['core_entity_2']].copy()\n",
    "\n",
    "'''\n",
    "将测试集和训练集一起放到模型中训练，\n",
    "其中测试集的样本权重设置为1，训练集为5\n",
    "'''\n",
    "w = np.array([1]*len(ddd) + [5]*len(d))\n",
    "def data_generator(data, targets, batch_size): \n",
    "    idx = np.arange(len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    batches = [idx[range(batch_size*i, min(len(data), batch_size*(i+1)))] for i in range(len(data)/batch_size+1)]\n",
    "    while True:\n",
    "        for i in batches:\n",
    "            xx, yy = np.array(map(gen_matrix, data[i])), np.array(map(gen_target, targets[i]))\n",
    "            yield (xx, yy, w[i])\n",
    "\n",
    "history = model.fit_generator(data_generator(\n",
    "                                    ddd[['words']].append(d[['words']], ignore_index=True)['words'], \n",
    "                                    ddd[['label']].append(d[['label']], ignore_index=True)['label'], \n",
    "                                    batch_size), \n",
    "                              samples_per_epoch=len(ddd)+len(d), \n",
    "                              nb_epoch=20)\n",
    "\n",
    "model.save_weights('words_seq2seq_final_3.model')\n",
    "d['predict'] = list(predict_data(d['words'], batch_size))\n",
    "dd['predict'] = list(predict_data(dd['words'], batch_size))\n",
    "dd['core_entity_3'] = map(predict, tqdm(iter(dd.index), desc=u'第二次迁移学习预测'))\n",
    "\n",
    "'''\n",
    "导出提交的JSON格式\n",
    "'''\n",
    "gen = lambda i:'[{\"content\": \"'+dd.iloc[i]['content']+'\", \"core_entity\": [\"'+''.join(dd.iloc[i]['core_entity_3'])+'\"]}]'\n",
    "ssss = map(gen, tqdm(range(len(dd))))\n",
    "result='\\n'.join(ssss)\n",
    "import codecs\n",
    "f=codecs.open('result3.txt', 'w', encoding='utf-8')\n",
    "f.write(result)\n",
    "f.close()\n",
    "import os\n",
    "os.system('rm result3.zip')\n",
    "os.system('zip result3.zip result3.txt')\n",
    "\n",
    "print u'正在进行第三次迁移学习......'\n",
    "\n",
    "'''\n",
    "开始迁移学习3。\n",
    "'''\n",
    "\n",
    "ddd = dd[dd['core_entity'] == dd['core_entity_2']].copy()\n",
    "ddd = ddd[ddd['core_entity_3'] == ddd['core_entity_2']].copy()\n",
    "\n",
    "'''\n",
    "将测试集和训练集一起放到模型中训练，\n",
    "其中测试集的样本权重设置为1，训练集为1\n",
    "'''\n",
    "w = np.array([1]*len(ddd) + [1]*len(d))\n",
    "def data_generator(data, targets, batch_size): \n",
    "    idx = np.arange(len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    batches = [idx[range(batch_size*i, min(len(data), batch_size*(i+1)))] for i in range(len(data)/batch_size+1)]\n",
    "    while True:\n",
    "        for i in batches:\n",
    "            xx, yy = np.array(map(gen_matrix, data[i])), np.array(map(gen_target, targets[i]))\n",
    "            yield (xx, yy, w[i])\n",
    "\n",
    "history = model.fit_generator(data_generator(\n",
    "                                    ddd[['words']].append(d[['words']], ignore_index=True)['words'], \n",
    "                                    ddd[['label']].append(d[['label']], ignore_index=True)['label'], \n",
    "                                    batch_size), \n",
    "                              samples_per_epoch=len(ddd)+len(d), \n",
    "                              nb_epoch=20)\n",
    "\n",
    "model.save_weights('words_seq2seq_final_4.model')\n",
    "d['predict'] = list(predict_data(d['words'], batch_size))\n",
    "dd['predict'] = list(predict_data(dd['words'], batch_size))\n",
    "dd['core_entity_4'] = map(predict, tqdm(iter(dd.index), desc=u'第三次迁移学习预测'))\n",
    "\n",
    "'''\n",
    "导出提交的JSON格式\n",
    "'''\n",
    "gen = lambda i:'[{\"content\": \"'+dd.iloc[i]['content']+'\", \"core_entity\": [\"'+''.join(dd.iloc[i]['core_entity_4'])+'\"]}]'\n",
    "ssss = map(gen, tqdm(range(len(dd))))\n",
    "result='\\n'.join(ssss)\n",
    "import codecs\n",
    "f=codecs.open('result4.txt', 'w', encoding='utf-8')\n",
    "f.write(result)\n",
    "f.close()\n",
    "import os\n",
    "os.system('rm result4.zip')\n",
    "os.system('zip result4.zip result4.txt')\n",
    "\n",
    "print u'正在进行第四次迁移学习......'\n",
    "\n",
    "'''\n",
    "开始迁移学习4。\n",
    "'''\n",
    "\n",
    "ddd = dd[dd['core_entity'] == dd['core_entity_2']].copy()\n",
    "ddd = ddd[ddd['core_entity_3'] == ddd['core_entity_2']].copy()\n",
    "ddd = ddd[ddd['core_entity_4'] == ddd['core_entity_2']].copy()\n",
    "\n",
    "'''\n",
    "将测试集和训练集一起放到模型中训练，\n",
    "其中测试集的样本权重设置为1，训练集为1\n",
    "'''\n",
    "w = np.array([1]*len(ddd) + [1]*len(d))\n",
    "def data_generator(data, targets, batch_size): \n",
    "    idx = np.arange(len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    batches = [idx[range(batch_size*i, min(len(data), batch_size*(i+1)))] for i in range(len(data)/batch_size+1)]\n",
    "    while True:\n",
    "        for i in batches:\n",
    "            xx, yy = np.array(map(gen_matrix, data[i])), np.array(map(gen_target, targets[i]))\n",
    "            yield (xx, yy, w[i])\n",
    "\n",
    "history = model.fit_generator(data_generator(\n",
    "                                    ddd[['words']].append(d[['words']], ignore_index=True)['words'], \n",
    "                                    ddd[['label']].append(d[['label']], ignore_index=True)['label'], \n",
    "                                    batch_size), \n",
    "                              samples_per_epoch=len(ddd)+len(d), \n",
    "                              nb_epoch=20)\n",
    "\n",
    "model.save_weights('words_seq2seq_final_5.model')\n",
    "d['predict'] = list(predict_data(d['words'], batch_size))\n",
    "dd['predict'] = list(predict_data(dd['words'], batch_size))\n",
    "dd['core_entity_5'] = map(predict, tqdm(iter(dd.index), desc=u'第四次迁移学习预测'))\n",
    "\n",
    "'''\n",
    "导出提交的JSON格式\n",
    "'''\n",
    "gen = lambda i:'[{\"content\": \"'+dd.iloc[i]['content']+'\", \"core_entity\": [\"'+''.join(dd.iloc[i]['core_entity_5'])+'\"]}]'\n",
    "ssss = map(gen, tqdm(range(len(dd))))\n",
    "result='\\n'.join(ssss)\n",
    "import codecs\n",
    "f=codecs.open('result5.txt', 'w', encoding='utf-8')\n",
    "f.write(result)\n",
    "f.close()\n",
    "import os\n",
    "os.system('rm result5.zip')\n",
    "os.system('zip result5.zip result5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
