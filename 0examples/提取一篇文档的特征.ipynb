{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何把一篇文档特征化？\n",
    "* 第一种方法：文档看做是words list，统计文档中出现词的个数作为该词的权重。  \n",
    "这里有个假设：word在doc中出现的越多，那这个word越能代表这个文档，那么它的权重应该越大，所以以word在doc中出现的次数来代表该word在doc中的权重     \n",
    "如许多文档组成的文集中出现了[w1,w2,w3,...]个word, doc1中[w1,w2,w3,...]分别出现了[1,0,3...]次，那么[1,0,3...]即是doc1的特征向量  \n",
    "\n",
    "* TF-IDF可以看做是对上述方法的改进   \n",
    "当然，这样并不准确，因为有的word在每个doc中都会出现很多次。为了减少这种误差，引入了TF-IDF   \n",
    "词频（TF）=某个词在文章中的出现次数  \n",
    "逆向文件频率 (inverse document frequency, IDF):  \n",
    "是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。  \n",
    "IDF = log2 (总文档数/含有这个词的文档数)\n",
    "\n",
    "TFIDF = TF x IDF 代替词频作为word 的权重\n",
    "\n",
    "sklearn中计算tfidf的公式：  https://github.com/scikit-learn/scikit-learn/blob/f0ab589f/sklearn/feature_extraction/text.py#L996\n",
    "tf-idf(d, t) = tf(t) * idf(d, t)  \n",
    "主要是idf计算的不同：  \n",
    "注意这里的log是自然对数！   \n",
    "标准的idf计算公式：idf(d, t) = log [ n / (df(d, t) + 1) ])  \n",
    "sklearn与之不同：  \n",
    "If ``smooth_idf=True`` (the default),  \n",
    "idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1  \n",
    "\n",
    "if ``smooth_idf=False``：\n",
    "idf(d, t) = log [ n / df(d, t) ] + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = ['I sed about sed the lack',\n",
    "          'of any Actually']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vocabulary list:\n",
      "\n",
      " {'sed': 5, 'about': 0, 'the': 6, 'lack': 3, 'of': 4, 'any': 2, 'actually': 1}\n"
     ]
    }
   ],
   "source": [
    "print ('\\nvocabulary list:\\n\\n',count_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t2\n",
      "  (0, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n"
     ]
    }
   ],
   "source": [
    "#(index1,index2) count中：index1表示为第几个句子或者文档，index2为所有语料库中的单词组成的词典的序号。\n",
    "#count为在这个文档中这个单词出现的次数。\n",
    "print (count_vec.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 2, 1],\n",
       "       [0, 1, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doc-token矩阵：每一行表示一个文档，每一列表示相应编号的token。值为token在doc中出现的频数。\n",
    "count_vec.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF  \n",
    "默认的token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b'，两个或两个以上的word，即不考虑单个汉字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'没有': 3, '地方': 1, '他乡': 0, '旅行': 2, '流浪': 4}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = ['没有 你 的 地方 都是 他乡 没有',\n",
    "          '没有 你 的 旅行 都是 流浪',\n",
    "          '没有 你 的 旅行 都是 流浪']\n",
    "stopwords=['都是']\n",
    "#norm:归一化，值为l2时，只把向量的长度=1，即sqrt(x1^2+x^2...)=1,值为l1，即abs(x1)+abs(x2)...=1\n",
    "tfidf=TfidfVectorizer(stop_words=stopwords)\n",
    "tfidf.fit(X_test)\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.37131279241563214\n",
      "  (0, 1)\t0.3143436037921839\n",
      "  (0, 0)\t0.3143436037921839\n",
      "  (1, 4)\t0.36015410466295694\n",
      "  (1, 3)\t0.27969179067408617\n",
      "  (1, 2)\t0.36015410466295694\n",
      "  (2, 4)\t0.36015410466295694\n",
      "  (2, 3)\t0.27969179067408617\n",
      "  (2, 2)\t0.36015410466295694\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6316672 , 0.6316672 , 0.        , 0.44943642, 0.        ],\n",
       "       [0.        , 0.        , 0.6316672 , 0.44943642, 0.6316672 ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6316672017376245, 0.6316672017376245, 0.0, 0.4494364165239821, 0.0],\n",
       " [0.0, 0.0, 0.6316672017376245, 0.4494364165239821, 0.6316672017376245]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.transform(X_test).todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.6316672 , 0.6316672 , 0.        , 0.44943642, 0.        ],\n",
       "        [0.        , 0.        , 0.6316672 , 0.44943642, 0.6316672 ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.transform(X_test).todense()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
